\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{subcaption}

\title{Unsupervised Task Discovery in Multi-Task Acoustic Modeling}

\name{Josh Meyer}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  University of Arizona}
\email{joshua.richard.meyer@gmail.com}

\begin{document}

\maketitle
% 
\begin{abstract}

  This study investigates low-resource acoustic modeling in Automatic Speech Recognition via Multi-Task Learning (MTL). The main question of this research is: \textit{How can we automatically discover useful auxiliary tasks to help train a neural network acoustic model?} Past work has already shown that linguist-crafted auxiliary tasks (either via acoustic landmarks or mapping to a source language) can help train more robust acoustic models in low-resource settings. However, to create these tasks the researcher must have access to expert linguistic knowledge. The following study demonstrates that WER in a low-resource setting can improve if the acoustic model is trained with an auxiliary task discovered via k-means clustering. Specifically, I train a Multi-Task DNN acoustic model, such that the model has multiple, separate output layers which represent (1) traditional phonemes defined by a phonetic decision tree or (2) clusters of audio discovered by standard k-means clustering. Given only 1.59 hours of audio, I observed a 1.66\% decrease in Word Error Rate when a second task was added during training. In an extremely limited data setting, I observed a .78\% decrease in WER. While these increases are small, this line of research promises easily scalable and unsupervised improvement in WER, and as such I believe warrants further exploration.

  
\end{abstract}

\noindent\textbf{Index Terms}: speech recognition, multi-task learning, acoustic modeling





\section{Introduction}

In the Multi-Task Learning (MTL) framework, data from a related task updates hidden layers in parallel with the target task \cite{caruana1997}. A task here is defined as a mapping of data to labels, and as such one can create a new task by creating new labels for existing data. In general it is difficult to create relevant labels for a new classification problem. The current study investigates auxiliary tasks (i.e. new labels) which are not hand-crafted by an expert or human, but automatically discovered from training data via unsupervised clustering (i.e. k-means).

The target language is Kyrgyz, and the data comes from an audiobook provided to the author by the Bizdin.kg project.

\begin{figure}[!htbp]
  \centering
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{figs/heigold-2013-dnn-c.png}
  \caption{Multi-Task Acoustic Model Architecture. Audio features are extracted via standard Kaldi scripts and then imported into TensorFlow. Standard k-means clustering is performed in TensorFlow, and the cluster identities are then exported back into Kaldi as targets for an auxiliary task. The final Multi-Task acoustic model is trained within Kaldi.}
    \label{fig:mtl-dnn}
  \endminipage\hfill
\end{figure}


\section{Background}

Past work on MTL for acoustic modeling can be divided into two main categories: monolingual vs. multilingual. Multilingual MTL acoustic modeling involves training a single DNN with multiple output layers, where each output layer represents triphones from one language. Monolingual MTL acoustic modeling involves designing multiple tasks for a single language, where each task is linguistically relevant (eg. triphones vs. monophones vs graphemes). Multilingual MTL aims for domain transfer, but monolingual MTL aims for robust generalization from the training data.

The earliest examples of multilingual MTL for ASR can be found in \cite{huang2013} and \cite{heigold2013}. These authors were interested in improving performance on all languages, not just one target language. During IARPA's Babel program, bottleneck MRASTA feature extraction was developed for low-resource languages, which relies on multi-task learning \cite{tuske2014multilingual}. More recently, \cite{grezl2016} studied the effect of adding data from a single, well-resourced language to some low-resourced language.

With regards to monolingual MTL, research has aimed to find tasks (from the same language) which are phonetically relevant to the main task \cite{bell2015}. The aim being to improve generalization to new data. Both \cite{seltzer2013} and later \cite{huang2015} looked at a very similar approach, defining additional auxiliary tasks in MTL via broad, abstract phonetic categories for English. With regards to low-resource languages, \cite{chen2014} and later \cite{chen2015} created extra tasks using graphemes or a universal phoneset as extra targets.

The current work falls into the Monolingual line of research on Multi-Task Learning for acoustic modeling.


\section{Experiments}


\subsection{Data}

The speech corpus used in the following experiments comes from an audiobook of a female speaker of Kyrgyz. A total of 1.59 hours of transcribed speech were used in training, and a held out 30 minutes were reserved for testing.


\subsection{Model Building}

All models are trained in Kaldi as \texttt{nnet3} Time-Delay Neural Networks (TDNNs) \cite{povey2011,peddinti2015}. The main neural net run script used in this paper can be found at www.github.com/JRMeyer/multi-task-kaldi/blob/master/mtk/run\_nnet3\_multitask.sh. The main GMM alignment script can be found at www.github.com/JRMeyer/multi-task-kaldi/blob/master/mtk/run\_gmm.sh.

During GMM alignment, monophones were allotted 1,000 Gaussian components, and trained over 25 iterations of EM. These monophones were then expanded into context-dependent triphones via a phonetic decision tree, with a maximum of 2,000 leaves \& 5,000 Gaussians. The resulting tied-state triphones are then trained over 25 iterations of EM. Given the alignments from the GMM-HMM models, a 5-layer, 1024-dimensional TDNN is trained over 2 epochs of backprop on a single GPU instance.

The auxiliary task (i.e. cluster labels from TensorFlow) is implemented as a separate output and penultimate layer. All other hidden layers of the TDNN are trained in parallel. A declining learning rate was used, with an initial $\alpha_{initial}=0.0015$ and a final $\alpha_{final}=0.00015$. A $ReLU$ activation function was used at every layer.

During testing, \textit{only} the main task is used. This highlights the purpose of the extra task: to force the learning of robust representations in the hidden layers during training; the auxiliary task serves as ``training wheels'' which are removed once the net is ready.


\subsubsection{Baseline Model}

The Single-Task baseline model has an identical architecture to the Multi-Task models without the additional task (5 hidden layers, 1024-dimensional layers, ReLU activations, same linear objective function).
\subsubsection{Auxiliary Task Discovery}

The new labels for the auxiliary task were discovered as such:

\begin{itemize}
\item Kaldi Feature Extraction
  \begin{itemize}
  \item 13 PLP features extracted via 25ms Hamming windows at a 10ms shift
  \item Resulting vectors spliced to have context of 16 frames to the left and 12 frames to the right (i.e. 29 frames per training example)
  \end{itemize}

\item TensorFlow k-means Clustering
  \begin{itemize}
  \item CMVN normalization applied to each training example
  \item Pre-set number of clusters discovered via TensorFlow's standard k-means clustering
  \item For each training example, its discovered cluster is assigned as new target label
  \end{itemize}

\item Mapping Kaldi targets onto TensorFlow clusters
   \begin{itemize}
   \item Given all training examples for a Kaldi target label, the most commonly assigned TensorFlow cluster is chosen as new target label. In this way, all training examples assigned the same label in Kaldi will share the same cluster from TensorFlow, however, the key addition is that multiple targets from Kaldi may be mapped onto a single TensorFlow cluster.
  \end{itemize} 
\end{itemize}






\subsection{Preliminary Results}


All results come from performance on the same held-out 30-minute section of Kyrgyz audiobook. Decoding is performed with a bigram backoff language model trained on a Wikipedia Kyrgyz dump, and contains, 103,998 unigrams and 56,6871 bigrams. The bigram language model, lexicon, and main-task decision tree are built into a standard decoding graph (ie. a Weighted Finite State Transducer) in the traditional Kaldi pipeline. 

The experimental results are shown in Table (\ref{tab:results}) as percent Word Error Rate (WER).


\begin{table}[!htbp]
  \centering
  \caption{Word Error Rates (WER\%)}
    \label{tab:results}
  \begin{adjustbox}{width=.4\textwidth}
    \begin{tabular}{lcc}
      \toprule
      & \multicolumn{2}{c}{ \textit{Amount of Training Data}}\\
      & 1.59 hours & 15 minutes \\
      \midrule
      STL Baseline                              & 49.56       &  83.51 \\
      \textbf{+ 250 TensorFlow Clusters}        & 48.88            & 83.71 \\
      \textbf{+ 500 TensorFlow Clusters}        & 47.90       & 82.83 \\
      \textbf{+ 1000 TensorFlow Clusters}        & 49.07       & 82.73 \\
      \midrule
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}




\section{Discussion}


When 1.59 hours of training data are used, every experimental condition shows improvement over the baseline, and 500 clusters shows the most improvement. In the extremely limited-data condition where only 15 minutes of data are used, two of the three experiments showed improvement over the baseline, and we find a trend where more clusters correlates to more improvement.








\section{Acknowledgements}

I'd like to thank Dan Povey for answering my (oftentimes naive) questions on the kaldi-help Google Group.

I'd like to also thank Chorobek Saadanbekov and Murat Jumashev for making the Kyrgyz audiobook available to me through the Bizdin.kg group.

This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. (DGE-1746060). Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation.





\newpage

\bibliographystyle{IEEEtran}
\bibliography{mybib}



\end{document}
%% ==> WER_nnet3_multitask_atai-15min-org_atai-15min-mod_tri_tri_1024_2epoch_1tohalf_1000clusters.txt <==
%% %WER 82.73 [ 848 / 1025, 9 ins, 262 del, 577 sub ] MTL/exp/nnet3/multitask/decode/wer_9

%% ==> WER_nnet3_multitask_atai-15min-org_atai-15min-mod_tri_tri_1024_2epoch_1tohalf_500clusters.txt <==
%% %WER 82.83 [ 849 / 1025, 7 ins, 265 del, 577 sub ] MTL/exp/nnet3/multitask/decode/wer_9

%% ==> WER_nnet3_multitask_atai-15min-org_atai-15min-mod_tri_tri_1024_2epoch_1tohalf_250clusters..txt <==
%% %WER 83.71 [ 858 / 1025, 9 ins, 283 del, 566 sub ] MTL/exp/nnet3/multitask/decode/wer_9

%% ==> WER_nnet3_multitask_atai-15min-org_tri_1024_2epoch_BASELINE.txt <==
%% %WER 83.51 [ 856 / 1025, 4 ins, 287 del, 565 sub ] MTL/exp/nnet3/multitask/decode/wer_9
