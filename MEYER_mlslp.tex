\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}

\title{Unsupervised Task Discovery in Multi-Task Acoustic Modeling}

\name{Josh Meyer}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  University of Arizona}
\email{joshua.richard.meyer@gmail.com}

\begin{document}

\maketitle
% 
\begin{abstract}

  This study investigates acoustic model training via Multi-Task Learning, where the auxiliary tasks are discovered in an unsupervised setting. Past work has shown that linguist-crafted auxiliary tasks can help train more robust acoustic models in low-resource settings. However, to create these tasks the researcher must have access to expert linguistic knowledge. The following study demonstrates that we can improve accuracy in a low-resource setting by training the acoustic model with an auxiliary task discovered via k-means clustering. Specifically, we train a Multi-Task DNN acoustic model, such that the model has two separate output layers which represent (1) traditional phonemes defined by a phonetic decision tree or (2) clusters of audio discovered by standard k-means clustering. Given only 1.59 hours of audio, we observed a 1.66\% improvement in Word Error Rate, and in an extremely limited data setting, we observed a .78\% improvement in WER. While these increases are small, this line of research promises easily scalable and unsupervised improvement in WER, and as such we believe warrants further exploration.

  
\end{abstract}

\noindent\textbf{Index Terms}: unsupervised learning, multi-task learning, acoustic modeling





\section{Introduction}

In the Multi-Task Learning (MTL) framework, data from a related task updates hidden layers in parallel with the target task \cite{caruana1997}. A task here is defined as a mapping of data to labels, and as such we can create a new task by creating new labels for existing data. In general it is difficult to create relevant labels for a new classification problem. The current study investigates auxiliary tasks which are not hand-crafted by an expert or human, but automatically discovered from training data via unsupervised clustering (i.e. k-means clustering). The cluster identities are then assigned as target labels for DNN training via backpropagation.


\begin{figure}[!htbp]
  \centering
\minipage{0.5\textwidth}
  \includegraphics[width=\linewidth]{figs/heigold-2013-dnn-c.png}
  \caption{Multi-Task Acoustic Model Architecture. Audio features are extracted via standard Kaldi scripts and then imported into TensorFlow. Standard k-means clustering is performed in TensorFlow, and the cluster identities are then exported back into Kaldi as targets for an auxiliary task. The final Multi-Task acoustic model is trained within Kaldi.}
    \label{fig:mtl-dnn}
  \endminipage\hfill
\end{figure}


\section{Background}

Past work in Mutli-Task acoustic modeling falls into two categories: monolingual vs. multilingual. Multilingual MTL acoustic modeling involves training a single DNN with multiple output layers, where each output layer represents triphones from a different language. Monolingual MTL acoustic modeling involves designing multiple tasks for a single language, where each task is a linguistically relevant classification: predicting triphones vs. predicting monophones vs. predicting graphemes. Multilingual MTL aims for language transfer, whereas monolingual MTL aims for generalization from the training data to unseen data.

%% The earliest examples of multilingual MTL for ASR can be found in \cite{huang2013} and \cite{heigold2013}. These authors were interested in improving performance on all languages, not just one target language. During IARPA's Babel program, bottleneck MRASTA feature extraction was developed for low-resource languages, which relies on multi-task learning \cite{tuske2014multilingual}. More recently, \cite{grezl2016} studied the effect of adding data from a single, well-resourced language to some low-resourced language.

With regards to monolingual MTL, research has aimed to find tasks (from the same language) which are phonetically relevant to the main task \cite{bell2015}. The aim being to improve generalization to new data. Both \cite{seltzer2013} and later \cite{huang2015} looked at a very similar approach, defining additional auxiliary tasks in MTL via broad, abstract phonetic categories for English. With regards to low-resource languages, \cite{chen2014} and later \cite{chen2015} created extra tasks using graphemes or a universal phoneset as extra targets.

The current work continues the monolingual line of research on Multi-Task Learning for acoustic modeling. We investigate automatic, unsupervised auxiliary task discovery in a low-resource setting.


\section{Experiments}


\subsection{Data}

The speech corpus used in the following experiments comes from an audiobook of a female speaker of Kyrgyz. A total of 1.59 hours of transcribed speech were used in training, and a held out 30 minutes were reserved for testing.


\subsection{Model Building}

Our experimental acoustic models are deep neural networks which have two separate output layers instead of just one (c.f. Figure (1)). The two output layers encode target labels for either (1) the main task (i.e. standard Kaldi triphone IDs) or (2) an auxiliary task (i.e. target labels which have been discovered via k-means clustering).

The intuition behind using k-means clustering is the following: unsupervised class discovery will surely lead to different classes compared to standard triphone GMM-HMM state alignment via the Baum-Welch algorithm. These k-means class assignments will be hopefully similar enough to the standard triphone labels that the clusters encode linguistically relevant categories. If the clusters are in fact linguistically relevant, then according to past work in MTL we should observe improved performance on the main task.

Instead of using raw k-means cluster labels for the auxiliary task, we first map all training examples which share a HMM triphone label to the same cluster. This ensures a certain degree of similarity between the main and auxiliary task. This effectively results in a merging of triphone labels, hopefully leading to a higher degree of linguistic abstraction.


\subsubsection{Auxiliary Task Discovery}

The new labels for the auxiliary task were discovered as such:

\begin{itemize}
\item Feature Extraction
  \begin{itemize}
  \item 13 PLP features extracted via 25ms Hamming windows at a 10ms shift
  \item Resulting vectors spliced to have context of 16 frames to the left and 12 frames to the right (i.e. 29 frames per training example)
  \end{itemize}

\item k-means Clustering
  \begin{itemize}
  \item CMVN normalization applied to each training example
  \item Pre-set number of clusters discovered via TensorFlow's standard k-means clustering
  \item For each training example, its discovered cluster is assigned as new target label
  \end{itemize}

\item Mapping triphone states onto k-means clusters
   \begin{itemize}
   \item Given all training examples for a given triphone state, the most commonly assigned k-means centroid is chosen as new target label for those examples. As such, training examples aligned to the same triphone will share the same k-means cluster, and examples from multiple triphones may be mapped onto the same cluster.
     
  \end{itemize} 
\end{itemize}

During GMM alignment, monophones were allotted 1,000 Gaussian components, and trained over 25 iterations of EM. These monophones were then expanded into context-dependent triphones via a phonetic decision tree, with a maximum of 2,000 leaves \& 5,000 Gaussians. The resulting tied-state triphones are then trained over 25 iterations of EM. The main GMM alignment script can be found on GitHub.\footnote{GMM alignment script: \url{www.github.com/JRMeyer/multi-task-kaldi/blob/master/mtk/run_gmm.sh}}.

Final models are trained in Kaldi as \texttt{nnet3} Time-Delay Neural Networks (TDNNs) via a cross-entropy objective function \cite{povey2011,peddinti2015}. Given the alignments from the GMM-HMM models, a 5-layer, 1024-dimensional TDNN is trained over 2 epochs of backprop on a single GPU instance. The main neural net run script used in this paper can be found on GitHub.\footnote{DNN training script: \url{www.github.com/JRMeyer/multi-task-kaldi/blob/master/mtk/run_nnet3_multitask.sh}}.

Each TDNN acoustic model is trained with two output tasks: (1) one output layer has standard context-dependent triphone targets, and (2) the other output layer has targets discovered via k-means clustering. As such, the auxiliary task (i.e. target labels discovered via k-means clustering) is implemented as a separate output and penultimate layer. All other hidden layers of the TDNN are trained in parallel. A declining learning rate was used, with an initial $\alpha_{initial}=0.0015$ and a final $\alpha_{final}=0.00015$. A $ReLU$ activation function was used at every layer. During testing, \textit{only} the main task is used. This highlights the purpose of the extra task: to force the learning of robust representations in the hidden layers during training; the auxiliary task serves as ``training wheels'' which are removed once the net is ready.


\subsubsection{Baseline Model}

The Single-Task baseline model has an identical architecture to the Multi-Task models without the additional task (5 hidden layers, 1024-dimensional layers, ReLU activations, same linear objective function).



\subsection{Preliminary Results}


All results come from performance on the same held-out 30-minute section of Kyrgyz audiobook. Decoding is performed with a bigram backoff language model trained on a Wikipedia Kyrgyz dump, and contains, 103,998 unigrams and 56,6871 bigrams. The bigram language model, lexicon, and main-task decision tree are built into a standard decoding graph (ie. a Weighted Finite State Transducer) in the traditional Kaldi pipeline. The experimental results are shown in Table (\ref{tab:results}) as percent Word Error Rate (WER).


\begin{table}[!htbp]
  \centering
  \caption{Word Error Rates (WER\%)}
    \label{tab:results}
  \begin{adjustbox}{width=.4\textwidth}
    \begin{tabular}{lcc}
      \toprule
      & \multicolumn{2}{c}{ \textit{Amount of Training Data}}\\
      & 1.59 hours & 15 minutes \\
      \midrule
      STL Baseline (context-dependent triphone targets)                            & 49.56       &  83.51 \\
      \textbf{+ 250 k-means cluster targets}        & 48.88            & 83.71 \\
      \textbf{+ 500 k-means cluster targets}        & 47.90       & 82.83 \\
      \textbf{+ 1000 k-means cluster targets}        & 49.07       & 82.73 \\
      \midrule
      \bottomrule
    \end{tabular}
  \end{adjustbox}
\end{table}




\section{Discussion}


When 1.59 hours of training data are used, every experimental condition shows improvement over the baseline, and 500 clusters shows the most improvement. In the extremely limited-data condition where only 15 minutes of data are used, two of the three experiments showed improvement over the baseline, and we find a trend where more clusters correlates to more improvement.

This approach warrants further investigation, because we already observe improvements with very little hyper-parameter tweaking. We plan to investigate this auxiliary task discovery by adjusting (1) number of clusters, (2) relative weightinf of main task to auxiliary task during backprop, (3) different feature projections before clustering, and other avenues.






\section{Acknowledgements}

I'd like to thank Dan Povey for answering my questions on the kaldi-help Google Group. I'd like to also thank Chorobek Saadanbekov and Murat Jumashev for making the Kyrgyz audiobook available to me through the Bizdin.kg group.

This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. (DGE-1746060). Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation.





\newpage

\bibliographystyle{IEEEtran}
\bibliography{mybib}



\end{document}
%% ==> WER_nnet3_multitask_atai-15min-org_atai-15min-mod_tri_tri_1024_2epoch_1tohalf_1000clusters.txt <==
%% %WER 82.73 [ 848 / 1025, 9 ins, 262 del, 577 sub ] MTL/exp/nnet3/multitask/decode/wer_9

%% ==> WER_nnet3_multitask_atai-15min-org_atai-15min-mod_tri_tri_1024_2epoch_1tohalf_500clusters.txt <==
%% %WER 82.83 [ 849 / 1025, 7 ins, 265 del, 577 sub ] MTL/exp/nnet3/multitask/decode/wer_9

%% ==> WER_nnet3_multitask_atai-15min-org_atai-15min-mod_tri_tri_1024_2epoch_1tohalf_250clusters..txt <==
%% %WER 83.71 [ 858 / 1025, 9 ins, 283 del, 566 sub ] MTL/exp/nnet3/multitask/decode/wer_9

%% ==> WER_nnet3_multitask_atai-15min-org_tri_1024_2epoch_BASELINE.txt <==
%% %WER 83.51 [ 856 / 1025, 4 ins, 287 del, 565 sub ] MTL/exp/nnet3/multitask/decode/wer_9
